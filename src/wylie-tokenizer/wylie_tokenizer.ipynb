{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "! pip install tokenizers transformers ipywidgets pandas datasets wandb huggingface_hub tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/tashitsering/Desktop/Work/model/wylie-tokenizer/.env/lib/python3.9/site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/tashitsering/Desktop/Work/model/wylie-tokenizer/.env/lib/python3.9/site-packages (from accelerate) (0.22.2)\n",
      "Collecting torch>=1.10.0\n",
      "  Downloading torch-2.3.0-cp39-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 61.0 MB 18.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /Users/tashitsering/Desktop/Work/model/wylie-tokenizer/.env/lib/python3.9/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/tashitsering/Desktop/Work/model/wylie-tokenizer/.env/lib/python3.9/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: pyyaml in /Users/tashitsering/Desktop/Work/model/wylie-tokenizer/.env/lib/python3.9/site-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: psutil in /Users/tashitsering/Desktop/Work/model/wylie-tokenizer/.env/lib/python3.9/site-packages (from accelerate) (5.9.8)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/tashitsering/Desktop/Work/model/wylie-tokenizer/.env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: filelock in /Users/tashitsering/Desktop/Work/model/wylie-tokenizer/.env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /Users/tashitsering/Desktop/Work/model/wylie-tokenizer/.env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 23.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.42.1 in /Users/tashitsering/Desktop/Work/model/wylie-tokenizer/.env/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: requests in /Users/tashitsering/Desktop/Work/model/wylie-tokenizer/.env/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.5-cp39-cp39-macosx_10_9_universal2.whl (18 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tashitsering/Desktop/Work/model/wylie-tokenizer/.env/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tashitsering/Desktop/Work/model/wylie-tokenizer/.env/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tashitsering/Desktop/Work/model/wylie-tokenizer/.env/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tashitsering/Desktop/Work/model/wylie-tokenizer/.env/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, MarkupSafe, sympy, networkx, jinja2, torch, accelerate\n",
      "Successfully installed MarkupSafe-2.1.5 accelerate-0.29.3 jinja2-3.1.3 mpmath-1.3.0 networkx-3.2.1 sympy-1.12 torch-2.3.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/tashitsering/Desktop/Work/model/wylie-tokenizer/.env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "from huggingface_hub import HfFolder, notebook_login\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 256000\n",
    "MAX_LEN    = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('ta4tsering/Tibetan_transliteration_wylie_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "for entry in dataset['train']:  # Adjust the slice as needed to limit data volume\n",
    "    text_data.append(entry['[text]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tshar chen blo gsal rgya mtsho'i gsung thor bu/__\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel as ByteLevelPreTokenizer\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = ByteLevelPreTokenizer()\n",
    "\n",
    "# Define the trainer with the desired specifications\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=VOCAB_SIZE,  # Customize the vocabulary size as needed\n",
    "    min_frequency=2,  # Minimum frequency for tokens\n",
    "    special_tokens=[\"<s>\", \"</s>\", \"<pad>\", \"<unk>\", \"<mask>\"]\n",
    ")\n",
    "\n",
    "# Now train the tokenizer\n",
    "tokenizer.train_from_iterator(text_data, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Disable parallelism to avoid potential deadlocks\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "tokenizer_folder = 'tokenizer'\n",
    "if not os.path.exists(tokenizer_folder):\n",
    "    os.makedirs(tokenizer_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer/vocab.json', 'tokenizer/merges.txt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save(f\"{tokenizer_folder}/tokenizer.json\")\n",
    "tokenizer.model.save(tokenizer_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
